{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Multi-Layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we explored some of the capabilities of `numpy` and built a Logistic Regression classifier. This week, you will build and run multi-layer (and deep) neural networks!\n",
    "\n",
    "We've broken this assignment up into two parts.\n",
    "- `01_step-by-step--building-a-deep-network` will walk you through implementing all the fuctions to build a deep network\n",
    "- `02_applying-a-dnn` will compose these functions into a deep network for image classification\n",
    "\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong>After this assignment, you should be able to:</strong>\n",
    "    <ul> \n",
    "        <li>Use non-linear units using activations functions such as ReLU to improve your model</li>\n",
    "        <li>Build multi-layer (deep) neural networks </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A quick note on notation:**\n",
    "- Superscript $[l]$ tells you a quantity associated with the $l^{th}$ layer\n",
    "  - e.g. $a^{[l]}$ is the $l^{th}$ layer's activation; $W^{[l]}$ and $b^{[l]}$ are the parameters: weight and bias for the $l^{th}$ layer.\n",
    "- Supercript $(i)$ is a quantity associated with the $i^{th}$ sample\n",
    "  - e.g. $x^{(i)}$ is the $i^{th}$ training sample\n",
    "- Subscript $i$ means the $i^{th}$ entry of a vector (of a sample)\n",
    "  - e.g. $a^{[l]}_i$ is the $i^{th}$ entry of the $l^{th}$ layer's activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Let's start with importing all the packages we'll use in this assignment.\n",
    "\n",
    "- [`numpy`](www.numpy.org) is a package for scientific computing with Python.\n",
    "- [`matplotlib`](http://matplotlib.org) is for plotting graphs in Python.\n",
    "- `p1_utils` provides some needed functions for this notebook.\n",
    "- `p1_tests` has some test cases to we'll use to assess the correctness of your functions\n",
    "- `np.random.seed(1)` is used for getting the same results after using random function calls. **This will help us grade your work – changing this will likely result in a failing grade.** _Don't do it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tests import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - What we'll do throughout the assignment\n",
    "\n",
    "You will first implement multiple \"helper functions.\" These functions will also be used in `02_applying-a-dnn` to build `2-layer` and `L-layer` neural networks. We'll walk you through each helper function you'll implement and have fairly detailed instructions on the necessary steps.\n",
    "\n",
    "An outline of what you will do in this assignment:\n",
    "- Initialize parameters for a `2-layer` and an `L-layer` neural network\n",
    "- Implement forward propagation (purple, in the figure below)\n",
    "  - Complete forward propagation's `linear` combination (this results in $Z^{[L]}$)\n",
    "  - (We'll give you the `activation` function – `ReLU`/`Sigmoid`)\n",
    "  - Combine the `linear` and `activation` steps into a single \\[`linear` -> `activation`\\] step\n",
    "  - Stack the function from the previous step into $L-1$ layers of \\[`linear` -> `ReLU`\\] and add a final \\[`linear` -> `Sigmoid`\\] layer at the end; this will result in `L_model_forward`\n",
    "- Compute the cost\n",
    "- Implement backpropagation (red, in the figure below)\n",
    "  - Complete backpropagation's `linear` combination (this results in the function: $Z^{[L]}$)\n",
    "  - (We'll give you the gradient of the `activation` function  – `ReLU_backward` / `Sigmoid_backward`)\n",
    "  - Combine the `linear` and `activation` steps into a single \\[`linear` -> `activation`\\] step\n",
    "  - Stack the function from the previous step into $L-1$ copies of \\[`linear` -> `ReLU`\\] and add \\[`linear` -> `Sigmoid`\\] for the final layer; this will result in the function: `L_model_backward`\n",
    "- Lastly, update the parameters (weights and bias values).\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>NOTE:</strong> Every forward function has a corresponding backward function. That's why, at every step of your forward propagation, you'll be storing some values in a cache. These cached values are needed to compute the gradients &ndash; in the backpropagation module, you'll then use the cache to calculate the gradients. We'll walk you through how to carry out each of these steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Initialization\n",
    "\n",
    "Below, you will implement two helper functions to initialize the parameters of your model.\n",
    "1. First one will be used to `initialize_parameters`\n",
    "2. Then, the second one will generalize `initialize_parameters_deep` to $L$ layers\n",
    "\n",
    "### 3.1 - Initialization for a `2-layer` Neural Network\n",
    "\n",
    "<div class=\"alert alert-info\"> <h2>Exercise 1:</h2>\n",
    "    <p> define and initialize the parameters of a <code>2-layer</code> neural network. </p>\n",
    "    <ul><strong>Instructions:</strong>\n",
    "        <li> The model structure should be: <code>linear -> relu -> linear -> sigmoid</code>.</li>\n",
    "        <li> Random initialization should be used for the weight matrices. (Hint: Use <code>np.random.randn(shape) * 0.01</code>.)</li>\n",
    "        <li> For the biases, use a zero initialization (Hint: use <code>np.zeros(shape)</code>).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (~4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"W2\": W2,\n",
    "                  \"b1\": b1, \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 values = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]\n",
      " [ 0.01744812 -0.00761207  0.00319039]]\n",
      "b1 values = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 values = [[-0.0024937   0.01462108 -0.02060141]]\n",
      "b2 values = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,3,1)\n",
    "print(\"W1 values = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 values = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 values = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 values = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "W1 values = [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]\n",
    " [ 0.01744812 -0.00761207  0.00319039]]\n",
    "b1 values = [[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W2 values = [[-0.0024937   0.01462108 -0.02060141]]\n",
    "b2 values = [[0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - Initialization of an `L-layer` Neural Network\n",
    "\n",
    "Initializating an `L-layer` neural network is more complex, largely because of the increased number of weight matrices and bias vectors. While completing the function: `initialize_parameters_deep` below, make sure that the dimensions between layers match! This is a common point of error, especially while learning. (Remember that, for example, $n^{[1]}$ means the number of units in Layer $1$.) If the size of the input $X$ is $(12288, 209)$ with $m=209$ examples, then...\n",
    "\n",
    "| Layer | `W.shape`                | `b.shape`        | Activation                                    |`activation.shape`|\n",
    "|:------:|--------------------------|------------------|-----------------------------------------------|------------------|\n",
    "|**1**   | $(n^{[1]}, 12288)$       | $(n^{[1]}, 1)$   | $Z^{[1]} = W^{[1]}  X + b^{[1]}$              | $(n^{[1]}, 209)$ |\n",
    "|**2**   | $(n^{[2]}, n^{[1]})$     | $(n^{[2]}, 1)$   | $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$         | $(n^{[2]}, 209)$ |\n",
    "|$\\vdots$| $\\vdots$                 | $\\vdots$         | $\\vdots$                                      | $\\vdots$         |\n",
    "|**L-1** | $(n^{[L-1]}, n^{[L-2]})$ | $(n^{[L-1]}, 1)$ | $Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ | $(n^{[L-1]}, 209)$ |\n",
    "|**L**   | $(n^{[L]}, n^{[L-1]})$   | $(n^{[L]}, 1)$   | $Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}$       | $(n^{[L]}, 209)$ |\n",
    "\n",
    "Recall, from PA1, that computing $W X + b$ in `numpy` will apply broadcasting; e.g. if:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <h2>Exercise 2:</h2>\n",
    "    <p> Implement a function to initialize the parameters for an <code>L-layer</code> neural network. </p>\n",
    "    <ul><strong>Instructions:</strong>\n",
    "        <li> The model's structure is <code>([linear -> relu] * (L-1)) ->linear -> sigmoid</code>. i.e. the model has $L-1$ layers using a <code>ReLU</code> activation, followed by an output layer with a <code>Sigmoid</code> activation.</li>\n",
    "        <li> Random initialization needs to be used for the weights. (Hint: Use <code>np.random.randn(shape) * 0.01</code>.)</li>\n",
    "        <li> For the biases, use a zero initialization (Hint: Use <code>np.zeros(shape)</code>).</li>\n",
    "        <li> We'll store $n^{[l]}$, the number of units in different layers, in <code>layer_dims</code>. </li>\n",
    "    </ul>\n",
    "</div> \n",
    "\n",
    "For example, consider a `3-layer` neural network where `layer_dims = [2, 4, 1]`. This corresponds to an input layer with 2 units, one hidden layer with 4 units, and 1 output layer with a single unit. This means that `W1.shape = (4, 2)`, `b1.shape = (4, 1)`, `W2.shape = (1, 4)`, and `b2.shape = (1,1)`; now, we'll generalize this to $L$ layers! \n",
    "\n",
    "The code below is an implementation for $L=1$ (a single-layer neural network); this should inspire you to implement the general case `L-layer` neural network.\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (~2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]\n",
      " [-0.01185047 -0.0020565   0.01486148  0.00236716 -0.01023785]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.00712993  0.00625245 -0.00160513 -0.00768836 -0.00230031]\n",
      " [ 0.00745056  0.01976111 -0.01244123 -0.00626417 -0.00803766]\n",
      " [-0.02419083 -0.00923792 -0.01023876  0.01123978 -0.00131914]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,5,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "```python\n",
    "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]\n",
    " [-0.01185047 -0.0020565   0.01486148  0.00236716 -0.01023785]]\n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W2 = [[-0.00712993  0.00625245 -0.00160513 -0.00768836 -0.00230031]\n",
    " [ 0.00745056  0.01976111 -0.01244123 -0.00626417 -0.00803766]\n",
    " [-0.02419083 -0.00923792 -0.01023876  0.01123978 -0.00131914]]\n",
    "b2 = [[0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Implementing Forward Propagation \n",
    "\n",
    "### 4.1 - `linear` Forward \n",
    "Now that we've initialized the parameters, we can compute the forward propagation. For that, you'll start by implementing some basic functions that we'll use later on when building the model. Complete the implementation of the followed steps, in order:\n",
    "\n",
    "1. `linear`\n",
    "2. `linear -> activation`, where `activation` can be either ReLU or Sigmoid\n",
    "3. `([linear -> ReLU]` $\\times$ `(L - 1)) -> linear -> sigmoid` (this is the whole model)\n",
    "\n",
    "The forward module (vectorized over all examples) computes the following equations:\n",
    "$$Z^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]}\\tag{4}$$ where $A^{[0]} = X$\n",
    "\n",
    "<div class=\"alert alert-info\"><h2> Exercise 3:</h2>\n",
    "    <p>Build the linear part of a layer for forward propagation.</p>\n",
    "    <p>\n",
    "        <strong>Reminder:</strong>\n",
    "        The mathematical expression of that for layer $l$ is $Z^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]}$. Hint: <code>np.dot</code> might be a useful here. Also, be sure to use (print) <code>W.shape</code> to verify dimensions, if you get errors related to the dims!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (~1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear part: Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"linear part: Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "```python\n",
    "linear part: Z = [[ 3.26295337 -1.23429987]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 -  `linear -> activation` Forward\n",
    "\n",
    "In this assignment, we'll be using two activation functions:\n",
    "\n",
    "1. **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. We've provided the `sigmoid` function for you. This function returns **two** items: the activation value `A` and a `cache` which contains `Z` (the `cache` is what we'll feed into the corresponding backward function). Use it like so:\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "1. **ReLU**: The mathematical representation is $A = ReLU(Z) = max(0, Z)$. We've provided the `relu` function for you. This function returns **two** items: the activation value `A` and a `cache` which contains `Z` (the `cache` is what we'll feed into the corresponding backward function). Use it like so:\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you'll group two functions (`linear` and `activation`) into a single function (`linear_activation`). This function will, then, do the `linear` forward step followed by an `activation` forward step.\n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 4</h2>\n",
    "    <p> Implement the forward propagation of the <code>linear -> activation</code> step. The mathematical formulation: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$ where the activation <code>g</code> can be <code>sigmoid</code> or <code>relu</code>. (Use the function <code>linear_forward</code> that you just implemented in the previous step and then choose the appropriate activation function.)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (~2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (~2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid : A = [[0.96890023 0.11013289]]\n",
      "With ReLU    : A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid : A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU    : A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "```python    \n",
    "With Sigmoid : A = [[ 0.96890023  0.11013289]]\n",
    "With ReLU    : A = [[ 3.43896131  0.        ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><strong>NOTE:</strong> In deep learning, we count <code>[LINEAR->ACTIVATION]</code> as a single layer, not as two.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) `L-Layer` Model \n",
    "\n",
    "For further convenience, when implementing an $L$-layer neural network, we'll need a function which replicates `linear_activation_forward` (with ReLU) $L-1$ times, then sets the last layer to `linear_activation_forward` (with Sigmoid).\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 5</h2>\n",
    "    <p>Implement the forward propagation for the above model.</p>\n",
    "    <p><strong>Instructions:</strong> In the code below, <code>AL</code> denotes $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is also typically called <code>yhat</code>, in papers, you'll likely see that as $\\hat{Y}$.)</p>\n",
    "</div>\n",
    "\n",
    "**Tips**:\n",
    "- Use the functions you've previously written\n",
    "- `for` loops to replicate `[linear -> relu]`, ($L-1$) times, are a good idea\n",
    "- Keep track of the `caches` in the `caches` list! We can add new values (say, `c`) to a `list` just by using `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (~2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (~2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)],parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "AL = [[ 0.03921668  0.70498921  0.19734387  0.04728177]]\n",
    "Length of caches list = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">Great! Now you have a fully-functional implementation of forward propagation that takes the input $X$ and outputs a row vector $A^{[L]}$ containing your predictions. It also records all the intermediate values in <code>caches</code>. Using $A^{[L]}$, we can compute the cost of your predictions.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "Now, we'll implement forward **_and_** backward propagation steps. You'll also compute the cost to see if your model is learning.\n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 6</h2>\n",
    "    <p>Compute cross-entropy cost $J$ for logistic regression, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (~1 lines of code)\n",
    "    cost = -1 / m *(np.dot(Y,np.log(AL.T)) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```python\n",
    "cost = 0.414931599615397\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "Similar to the forward propagation, you'll also implement helper functions for the backpropagation (backprop). **Remember** backprop is used to calculate the derivatives of the cost function, with respect to the parameters so that we can update the parameters. \n",
    "\n",
    "**Reminder**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "Similar to forward propagation, we'll be building backprop in three steps:\n",
    "1. `linear` backward\n",
    "1. `linear -> activation` backward, where `activation` computes the derivative of either `ReLU` or `Sigmoid` activations\n",
    "1. `([linear -> relu]` $\\times$ `(L - 1)) -> linear -> sigmoid_backward` (this is the complete model for our L-layer neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you've already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Then, after that, you'll want to calculate the values: $dW^{[l]}, db^{[l]}$ and $dA^{[l-1]}$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "Three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$. Below, you'll find the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h2>Exercise 7</h2>\n",
    "    <p>Use the given 3 formulas above and complete the implemention of the function: <code>linear_backward</code> below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (~3 lines of code)\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * (np.sum(dZ,axis = 1,keepdims = True))\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "```python\n",
    "dA_prev = [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]]\n",
    "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
    "db = [[ 0.50629448]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "Next, we'll merge the two helper functions `linear_backward` and `linear_activation_backward`. To ease this, a bit, we've provided two `backward` functions:\n",
    "1. **`sigmoid_backward`**: which implements the backprop for the Sigmoid function. Here's how to use it:\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, Z_cache)\n",
    "```\n",
    "1. **`relu_backward`**: which implements backprop for the ReLU function. Here's how to use it:\n",
    "```python\n",
    "dZ = relu_backward(dA, Z_cache)\n",
    "```\n",
    "\n",
    "Considering that the $g(.)$ is the activation function, use `sigmoid_backward` and `relu_backward` to compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 8</h2>\n",
    "    <p> Complete the below implementation of backprop for the <code>linear -> activation</code> layer. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (~2 lines of code)\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (~2 lines of code)\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation=\"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation=\"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "sigmoid:\n",
    "dA_prev = [[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]]\n",
    "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
    "db = [[-0.05729622]]\n",
    "\n",
    "relu:\n",
    "dA_prev = [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]]\n",
    "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
    "db = [[-0.20837892]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "\n",
    "Now, we'll implement the backward funcntion for the whole network. When you implemented `L_model_forward`, at each iteration there was a `cache` of `(X, W, b, z)`. In the backprop module, we'll use those variables to compute the derivatives (also called gradients in the NN literature); therefore, in `L_model_backward`, you'll iterate through all the hidden layers backward, starting from layer $L$. At each step, you'll use the cached values for layer $l$ to backprop through layer $l$. Look below, at Figure 5, to see a backward pass.\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Initializing backpropagation**:\n",
    "To backpropagate through this network, we know that the output is, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Which means your code needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "Then, this post-activation gradient (`dAL`) can be used to keep going backward. As seen in Figure 5, you can now feed `dAL` into the `linear -> sigmoid` backward function, which will use the cached values stored by `L_model_forward`). Afterwards, you'll have to use a `for` loop to iterate throughall the other layers using `linear -> relu`'s backward function. At each layer, you should store `dA`, `dW`, and `db` in the `grads` dict. To standardize things, use this format:\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "\n",
    "For example, for $l=2$, this would store $dW^{[l]}$ in `grads[\"dW2\"]`.\n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 9</h2>\n",
    "    <p> Implement backprop for <code>([linear -> relu]</code> $\\times$ <code>(L-1)) -> linear -> sigmoid</code> model. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (~2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (~5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```python\n",
    "dW1 = [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]]\n",
    "db1 = [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]]\n",
    "dA1 = [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h2>Exercise 10</h2>\n",
    "<p> Complete the implementation of <code>update_parameters</code> to update your parameters with gradient descent.</p>\n",
    "\n",
    "<p><strong>Instructions:</strong> Update all the parameters: $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$ by using gradient descent algorithm. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (~3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+ str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+ str(l+1)]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.43464413 -0.06063193 -2.13716107  1.65890574]\n",
      " [-1.7906617  -0.83819978  0.50370883 -1.23901808]\n",
      " [-1.05751404 -0.90423543  0.56459269  2.28336179]]\n",
      "b1 = [[ 0.03272621]\n",
      " [-1.13502118]\n",
      " [ 0.53855798]]\n",
      "W2 = [[-0.59211293 -0.0136769   1.19046599]]\n",
      "b2 = [[-0.75769462]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.01)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```python\n",
    "W1 = [[-0.43464413 -0.06063193 -2.13716107  1.65890574]\n",
    " [-1.7906617  -0.83819978  0.50370883 -1.23901808]\n",
    " [-1.05751404 -0.90423543  0.56459269  2.28336179]]\n",
    "b1 = [[ 0.03272621]\n",
    " [-1.13502118]\n",
    " [ 0.53855798]]\n",
    "W2 = [[-0.59211293 -0.0136769   1.19046599]]\n",
    "b2 = [[-0.75769462]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Conclusion\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <p>You implemented forward and backward computations for L-layer neural networks! </p>\n",
    "</div>\n",
    "\n",
    "Please switch to the part 2 now (file: `02_applying-a-dnn`) to use these implementations in cat classification.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<p><code>02_applying-a-dnn</code> involves combining all these functions to build two models:</p>\n",
    "<ol>\n",
    "    <li>a 2-layer neural network</li>\n",
    "    <li>an L-layer neural network</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "We'll actually be using these models to classify images as being cat or non-cat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
